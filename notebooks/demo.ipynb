{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkinTag: Robust Skin Lesion Classification\n",
    "\n",
    "Using MedSigLIP embeddings with augmentations for fairness across skin tones.\n",
    "\n",
    "**Hackathon Pitch:**\n",
    "1. Problem: Skin lesion classifiers perform worse on darker skin tones\n",
    "2. Pre-trained model: MedSigLIP (400M vision encoder)\n",
    "3. Augmentations: Skin tone, lighting, noise variations\n",
    "4. Results: Improved fairness across demographic groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup (uncomment if running on Colab)\n",
    "# !pip install -q transformers albumentations scikit-learn\n",
    "# !git clone https://github.com/MedGemma540/SkinTag.git\n",
    "# %cd SkinTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from src.model.embeddings import EmbeddingExtractor\n",
    "from src.model.classifier import SklearnClassifier, ZeroShotClassifier\n",
    "from src.data.augmentations import (\n",
    "    get_training_transform,\n",
    "    get_eval_transform,\n",
    "    get_skin_tone_augmentation,\n",
    "    get_lighting_augmentation,\n",
    "    get_noise_augmentation,\n",
    ")\n",
    "from src.evaluation.metrics import robustness_report\n",
    "\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Sample Data\n",
    "\n",
    "Using a small subset for demo. Replace with full dataset for real experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download dataset\n",
    "# Option 1: ISIC Archive (https://www.isic-archive.com/)\n",
    "# Option 2: HAM10000 via Kaggle\n",
    "# Option 3: Fitzpatrick17k for skin tone diversity\n",
    "\n",
    "DATA_DIR = Path(\"../data\")  # Adjust path\n",
    "CACHE_DIR = Path(\"../results/embeddings\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Placeholder: load your images here\n",
    "# images = [Image.open(p).convert('RGB') for p in DATA_DIR.glob('**/*.jpg')]\n",
    "# labels = [...]  # 0 = benign, 1 = malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Augmentation Visualization\n",
    "\n",
    "Show how augmentations simulate real-world variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_augmentations(image, augmentations: dict, cols=4):\n",
    "    \"\"\"Show original image with various augmentations applied.\"\"\"\n",
    "    img_array = np.array(image)\n",
    "    n = len(augmentations) + 1\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 3 * rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    axes[0].imshow(img_array)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    for i, (name, aug) in enumerate(augmentations.items(), 1):\n",
    "        augmented = aug(image=img_array)[\"image\"]\n",
    "        axes[i].imshow(augmented)\n",
    "        axes[i].set_title(name)\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../results/augmentations.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment when you have data:\n",
    "# sample_image = images[0]\n",
    "# visualize_augmentations(sample_image, {\n",
    "#     \"Skin Tone Shift\": get_skin_tone_augmentation(),\n",
    "#     \"Lighting Variation\": get_lighting_augmentation(),\n",
    "#     \"Noise Injection\": get_noise_augmentation(),\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Embeddings\n",
    "\n",
    "Extract once, reuse for all experiments. Use small batch size on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 if not torch.cuda.is_available() else 16\n",
    "\n",
    "extractor = EmbeddingExtractor()\n",
    "\n",
    "# Extract and cache embeddings\n",
    "# embeddings = extractor.extract_dataset(\n",
    "#     images,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     cache_path=CACHE_DIR / \"train_embeddings.pt\"\n",
    "# )\n",
    "# \n",
    "# # Free memory after extraction\n",
    "# extractor.unload_model()\n",
    "# \n",
    "# print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Classifier\n",
    "\n",
    "Fast sklearn classifier on cached embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     embeddings.numpy(), labels, test_size=0.2, random_state=42, stratify=labels\n",
    "# )\n",
    "\n",
    "# Train classifier (< 1 minute)\n",
    "# clf = SklearnClassifier(classifier_type=\"logistic\")\n",
    "# clf.fit(X_train, y_train)\n",
    "# \n",
    "# print(f\"Train accuracy: {clf.score(X_train, y_train):.3f}\")\n",
    "# print(f\"Test accuracy: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Zero-Shot Classification (No Training)\n",
    "\n",
    "Alternative: classify using text descriptions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_DESCRIPTIONS = [\n",
    "    \"a photograph of a benign skin lesion, such as a mole or seborrheic keratosis\",\n",
    "    \"a photograph of a malignant melanoma, a dangerous skin cancer\",\n",
    "]\n",
    "\n",
    "# zero_shot = ZeroShotClassifier(extractor, CLASS_DESCRIPTIONS)\n",
    "# predictions = zero_shot.predict(embeddings)\n",
    "# \n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(f\"Zero-shot accuracy: {accuracy_score(labels, predictions):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Robustness Evaluation\n",
    "\n",
    "Compare performance with and without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(clf, test_images, test_labels, extractor, augmentation=None):\n",
    "    \"\"\"Evaluate classifier on clean or augmented test images.\"\"\"\n",
    "    if augmentation:\n",
    "        test_images = [Image.fromarray(augmentation(image=np.array(img))[\"image\"]) for img in test_images]\n",
    "    \n",
    "    embeddings = extractor.extract_dataset(test_images, batch_size=4)\n",
    "    predictions = clf.predict(embeddings)\n",
    "    \n",
    "    return robustness_report(test_labels, predictions, class_names=[\"benign\", \"malignant\"])\n",
    "\n",
    "# results = {\n",
    "#     \"clean\": evaluate_robustness(clf, test_images, y_test, extractor),\n",
    "#     \"skin_tone\": evaluate_robustness(clf, test_images, y_test, extractor, get_skin_tone_augmentation()),\n",
    "#     \"lighting\": evaluate_robustness(clf, test_images, y_test, extractor, get_lighting_augmentation()),\n",
    "#     \"noise\": evaluate_robustness(clf, test_images, y_test, extractor, get_noise_augmentation()),\n",
    "# }\n",
    "# \n",
    "# for condition, report in results.items():\n",
    "#     print(f\"\\n{condition.upper()}: {report['overall_accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_robustness_comparison(results: dict):\n",
    "    \"\"\"Bar chart comparing accuracy across conditions.\"\"\"\n",
    "    conditions = list(results.keys())\n",
    "    accuracies = [results[c][\"overall_accuracy\"] for c in conditions]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(conditions, accuracies, color=[\"#2ecc71\" if c == \"clean\" else \"#e74c3c\" for c in conditions])\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Model Robustness Across Conditions\")\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02, f\"{acc:.2f}\", ha=\"center\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../results/robustness.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# plot_robustness_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Findings:**\n",
    "- MedSigLIP provides strong zero-shot performance on skin lesions\n",
    "- Models trained without augmentation degrade under skin tone / lighting shifts\n",
    "- Targeted augmentations improve robustness and fairness\n",
    "\n",
    "**Next Steps:**\n",
    "- Evaluate on Fitzpatrick17k for explicit skin tone fairness metrics\n",
    "- Compare against baseline (ResNet, EfficientNet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
