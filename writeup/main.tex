\documentclass{article}

% NeurIPS 2024 style (use neurips_2025 when available)
\usepackage[final]{neurips_2024}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amsmath}

\title{SkinTag: Domain-Robust and Fairness-Aware Skin Lesion Triage via Fine-Tuned Vision-Language Models}

\author{%
  MedGemma540 Team \\
  % Institution \\
  % \texttt{email@institution.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Dermatology AI systems trained on dermoscopic images consistently underperform on clinical photographs, particularly for patients with darker skin tones---the populations with least access to dermatologists. We present \textbf{SkinTag}, a skin lesion triage system that addresses both domain shift and skin tone bias through multi-dataset training across five complementary datasets spanning dermoscopic, clinical, and smartphone imaging domains. Our approach fine-tunes SigLIP, a vision-language foundation model, with domain-balanced and Fitzpatrick-balanced sampling weights, augmented with realistic field condition distortions (blur, noise, compression, lighting variation). On a held-out test set of 9,456 images, our best model---XGBoost trained on fine-tuned SigLIP embeddings---achieves \textbf{96.8\% accuracy} with F1-malignant of 0.922 and AUC of 0.992, while maintaining equalized odds gaps below 0.05 for sensitivity across Fitzpatrick skin types. We introduce a four-tier clinical triage system (URGENT/PRIORITY/ROUTINE/MONITOR) with condition-specific guidance designed for real-world deployment. For mobile use in low-resource settings, we apply knowledge distillation to compress the model by 200$\times$ (from 3.4 GB to 12 MB) while preserving accuracy, enabling fully offline inference on consumer smartphones.
\end{abstract}

\section{Introduction}

Skin cancer is the most common cancer worldwide, with melanoma alone causing over 57,000 deaths annually \citep{sung2021global}. Early detection dramatically improves outcomes: 5-year survival for localized melanoma exceeds 99\%, but drops to 32\% for distant metastases \citep{siegel2023cancer}. However, access to dermatologists is severely limited---in the United States, the average wait time for a dermatology appointment exceeds one month, and in many low-resource settings, dermatologists are virtually unavailable \citep{kimball2008national}.

Artificial intelligence offers a potential solution: automated screening systems that can triage skin lesions and identify those requiring urgent professional evaluation. Recent advances in deep learning have achieved dermatologist-level performance on curated dermoscopic datasets \citep{esteva2017dermatologist, haenssle2018man}. However, two critical limitations prevent deployment in real-world settings:

\textbf{Domain shift.} Most AI systems are trained on dermoscopic images---high-quality photographs taken with specialized equipment in clinical settings. However, patients seeking screening typically have access only to consumer smartphone cameras. Models trained on dermoscopic images fail catastrophically on clinical photographs \citep{daneshjou2022disparities}.

\textbf{Skin tone bias.} Dermatology datasets are overwhelmingly composed of lighter-skinned patients. The HAM10000 dataset \citep{tschandl2018ham10000}, a benchmark in the field, contains no Fitzpatrick skin type annotations. Studies have shown that AI systems trained on such data exhibit significantly lower sensitivity on darker skin tones \citep{daneshjou2022disparities, kinyanjui2020fairness}, precisely the populations with worst access to dermatological care.

We present \textbf{SkinTag}, a skin lesion triage system designed to address both challenges. Our contributions are:

\begin{enumerate}
    \item \textbf{Multi-domain training}: We aggregate five complementary datasets spanning dermoscopic, clinical, and smartphone imaging domains (47,277 images total), enabling robust generalization across acquisition conditions.

    \item \textbf{Fairness-aware fine-tuning}: We fine-tune SigLIP \citep{zhai2023sigmoid}, a state-of-the-art vision-language model, with combined domain and Fitzpatrick-balanced sampling weights, explicitly upweighting underrepresented skin tones.

    \item \textbf{Comprehensive fairness evaluation}: We report equalized odds gaps across Fitzpatrick skin types, imaging domains, and demographic groups, demonstrating substantial fairness improvements over baseline approaches.

    \item \textbf{Triage-appropriate outputs}: Rather than diagnostic predictions, our system provides urgency tiers suitable for screening: low concern, moderate concern (schedule appointment), and high concern (seek prompt evaluation).
\end{enumerate}

\section{Related Work}

\subsection{Deep Learning for Dermatology}

The application of deep learning to dermatological diagnosis began with \citet{esteva2017dermatologist}, who demonstrated that a CNN trained on 129,450 clinical images achieved dermatologist-level accuracy on skin cancer classification. Subsequent work has refined these approaches using dermoscopic images \citep{haenssle2018man, brinker2019deep}, achieving impressive performance on curated benchmarks.

However, \citet{daneshjou2022disparities} demonstrated that these systems exhibit significant performance disparities across skin tones and imaging conditions. Their Diverse Dermatology Images (DDI) dataset, deliberately balanced across Fitzpatrick skin types, revealed that models trained on dermoscopic data performed substantially worse on clinical images from darker-skinned patients.

\subsection{Vision-Language Foundation Models}

Recent vision-language models pretrained on web-scale image-text pairs have shown remarkable transfer learning capabilities. CLIP \citep{radford2021learning} demonstrated that contrastive pretraining enables zero-shot transfer to diverse downstream tasks. SigLIP \citep{zhai2023sigmoid} improved upon CLIP by replacing the softmax cross-entropy loss with a sigmoid loss, enabling better scaling and performance.

For medical imaging, foundation models have shown promise in leveraging visual knowledge from general domains \citep{zhang2023biomedclip}. However, their application to dermatology with explicit fairness considerations remains underexplored.

\subsection{Fairness in Medical AI}

Algorithmic fairness in healthcare has received increasing attention following \citet{obermeyer2019dissecting}, who demonstrated racial bias in a widely-used healthcare algorithm. In dermatology specifically, \citet{kinyanjui2020fairness} showed that skin tone significantly affects classifier performance, with darker skin types exhibiting lower accuracy.

The equalized odds criterion \citep{hardt2016equality} requires that a classifier have equal true positive and false positive rates across protected groups. We adopt this metric for our fairness evaluation, measuring the maximum gap in sensitivity and specificity across Fitzpatrick skin types.

\section{Datasets}

We aggregate five publicly available dermatology datasets, selected to maximize diversity in imaging domains, skin tones, and geographic origins. Table~\ref{tab:datasets} summarizes their characteristics.

\begin{table}[h]
\centering
\caption{Dataset characteristics. FST = Fitzpatrick Skin Type annotations available.}
\label{tab:datasets}
\begin{tabular}{lrllll}
\toprule
\textbf{Dataset} & \textbf{Images} & \textbf{Domain} & \textbf{FST} & \textbf{Origin} & \textbf{Labels} \\
\midrule
HAM10000 \citep{tschandl2018ham10000} & 10,015 & Dermoscopic & No & Austria & 7 classes \\
DDI \citep{daneshjou2022disparities} & 656 & Clinical & Yes & USA & Binary \\
Fitzpatrick17k \citep{groh2021evaluating} & 16,518 & Clinical & Yes & Web & 114 conditions \\
PAD-UFES-20 \citep{pacheco2020pad} & 2,298 & Smartphone & Yes & Brazil & 6 classes \\
BCN20000 \citep{combalia2024bcn20000} & 17,790 & Dermoscopic & No & Spain & 8 classes \\
\midrule
\textbf{Total} & \textbf{47,277} & & & & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Label Harmonization}

Each dataset uses different diagnostic taxonomies. We harmonize labels into two classification targets:

\textbf{Binary triage} (benign/malignant): The primary output for screening. Malignant includes melanoma, basal cell carcinoma, squamous cell carcinoma, and actinic keratosis. Non-neoplastic conditions (e.g., eczema, psoriasis) are included as benign---for triage purposes, ``not cancer'' is the relevant determination.

\textbf{Condition estimation} (10 classes): A secondary output providing the most likely specific condition: melanoma, BCC, SCC, actinic keratosis, melanocytic nevus, seborrheic keratosis, dermatofibroma, vascular lesion, non-neoplastic, and other/unknown.

\subsection{Domain and Skin Tone Distribution}

The aggregated dataset spans three imaging domains: dermoscopic (59\%), clinical (36\%), and smartphone (5\%). Fitzpatrick skin type annotations are available for 41\% of images (from DDI, Fitzpatrick17k, and PAD-UFES-20), with the distribution skewed toward lighter skin types (I-III: 68\%, IV-VI: 32\%).

\section{Methods}

\subsection{Model Architecture}

We use SigLIP-SO400M \citep{zhai2023sigmoid} as our visual backbone, a 400M parameter vision transformer pretrained on 400M image-text pairs using sigmoid contrastive loss. The model produces 1152-dimensional embeddings for 384$\times$384 pixel images.

For fine-tuning, we add a classification head consisting of:
\begin{itemize}
    \item Batch normalization
    \item Linear layer (1152 $\rightarrow$ 256) with ReLU activation
    \item Dropout (p=0.3)
    \item Linear layer (256 $\rightarrow$ 2) for binary classification
\end{itemize}

We unfreeze the last 4 transformer layers of the vision encoder for end-to-end fine-tuning, keeping earlier layers frozen to preserve general visual features.

\subsection{Fairness-Aware Sampling}

To address both domain shift and skin tone bias, we compute sample weights that balance across two dimensions simultaneously:

\textbf{Domain balancing}: Each imaging domain (dermoscopic, clinical, smartphone) contributes equally to training loss, regardless of dataset size.

\textbf{Fitzpatrick balancing}: Within each domain, samples are weighted inversely proportional to the frequency of their (Fitzpatrick type, label) combination. For images without Fitzpatrick annotations, we assign the median weight.

The combined weight for sample $i$ is:
\begin{equation}
w_i = w_{\text{domain}}(d_i) \cdot w_{\text{fitz}}(f_i, y_i)
\end{equation}
where $d_i$ is the domain, $f_i$ is the Fitzpatrick type, and $y_i$ is the label.

\subsection{Training Protocol}

We split the data 80/20 into training (37,821) and test (9,456) sets, stratified by dataset and label. Training uses:
\begin{itemize}
    \item AdamW optimizer with learning rate $10^{-4}$
    \item Batch size 16
    \item 10 epochs with early stopping (patience 3)
    \item Binary cross-entropy loss with sample weights
\end{itemize}

\subsection{Baseline Models}

We compare against several baselines using frozen SigLIP embeddings:
\begin{itemize}
    \item \textbf{Majority class}: Always predicts benign (floor for accuracy)
    \item \textbf{Logistic regression}: StandardScaler + LogisticRegression
    \item \textbf{XGBoost}: Gradient boosted trees on embeddings
    \item \textbf{Deep MLP}: 2-layer MLP with identical architecture to our classification head, but trained on frozen embeddings
\end{itemize}

\subsection{Field Condition Augmentations}

To improve robustness to real-world smartphone capture conditions, we apply augmentations simulating common image degradations \citep{buslaev2020albumentations, hendrycks2019benchmarking}:

\begin{itemize}
    \item \textbf{Motion blur}: Simulates shaky hands during capture (MotionBlur, GaussianBlur)
    \item \textbf{Focus issues}: Autofocus problems common in macro photography (Defocus, ZoomBlur)
    \item \textbf{Lighting variation}: Indoor/outdoor, different light sources (RandomBrightnessContrast, RandomGamma)
    \item \textbf{Color cast}: Fluorescent lights, sunlight variations (HueSaturationValue, RGBShift)
    \item \textbf{Sensor noise}: Low light capture, older phone cameras (GaussNoise, ISONoise)
    \item \textbf{Compression artifacts}: WhatsApp, social media sharing (ImageCompression, Downscale)
    \item \textbf{Shadows and glare}: Hand occlusion, flash reflection (RandomShadow, RandomSunFlare)
\end{itemize}

We apply these augmentations with probability 0.3--0.5 during training, with severity levels (light, moderate, heavy) configurable for different deployment environments.

\subsection{Clinical Triage System}

Rather than outputting raw probabilities, we provide clinically actionable triage tiers designed with dermatologist input \citep{janda2020can}:

\begin{table}[h]
\centering
\caption{Four-tier clinical triage system with recommended action timeframes.}
\label{tab:triage}
\begin{tabular}{llll}
\toprule
\textbf{Tier} & \textbf{Score} & \textbf{Conditions} & \textbf{Action} \\
\midrule
URGENT & $>60\%$ & Melanoma, SCC & Dermatology within 2 weeks \\
PRIORITY & 30--60\% & BCC, Actinic Keratosis & Dermatology within 1 month \\
ROUTINE & 15--30\% & Non-Neoplastic & Primary care within 3 months \\
MONITOR & $<15\%$ & Benign lesions & Self-monitor 6--12 months \\
\bottomrule
\end{tabular}
\end{table}

The thresholds were optimized for 95\% sensitivity on malignant conditions, yielding 77\% specificity at the binary threshold of 0.157. Each tier includes condition-specific guidance with links to peer-reviewed dermatology resources \citep{dermnetnz}.

\section{Results}

\subsection{Overall Performance}

Table~\ref{tab:results} presents the main results on the held-out test set.

\begin{table}[h]
\centering
\caption{Binary classification performance on test set (n=9,456). Embedding type indicates whether the model uses frozen SigLIP embeddings, fine-tuned embeddings, or end-to-end training.}
\label{tab:results}
\begin{tabular}{llccccc}
\toprule
\textbf{Model} & \textbf{Emb. Type} & \textbf{Accuracy} & \textbf{F1 Macro} & \textbf{F1 Malig.} & \textbf{AUC} \\
\midrule
Majority baseline & N/A & 0.791 & 0.442 & 0.000 & 0.500 \\
Logistic regression & Frozen & 0.840 & 0.792 & 0.692 & 0.922 \\
XGBoost & Frozen & 0.957 & 0.938 & 0.903 & 0.990 \\
Fine-tuned SigLIP & End-to-end & 0.962 & 0.945 & 0.914 & 0.990 \\
\midrule
\textbf{XGBoost} & \textbf{Fine-tuned} & \textbf{0.968} & \textbf{0.951} & \textbf{0.922} & \textbf{0.992} \\
\bottomrule
\end{tabular}
\end{table}

The best performing approach combines fine-tuned embeddings with gradient boosting: XGBoost on fine-tuned SigLIP embeddings achieves 96.8\% accuracy and 0.922 F1-malignant, outperforming both end-to-end fine-tuning (96.2\%) and XGBoost on frozen embeddings (95.7\%). This two-stage approach leverages fine-tuning to improve embedding quality while using XGBoost's robustness for classification.

\subsection{Fairness Analysis}

Table~\ref{tab:fairness} presents equalized odds gaps across protected attributes.

\begin{table}[h]
\centering
\caption{Equalized odds gaps (maximum difference across groups) for XGBoost model.}
\label{tab:fairness}
\begin{tabular}{lccc}
\toprule
\textbf{Attribute} & \textbf{Sensitivity Gap} & \textbf{Specificity Gap} & \textbf{F1 Gap} \\
\midrule
Fitzpatrick skin type & 0.044 & 0.091 & 0.111 \\
Imaging domain & 0.033 & 0.064 & 0.127 \\
Sex & 0.035 & 0.035 & 0.013 \\
Age group & 0.044 & 0.165 & 0.167 \\
\bottomrule
\end{tabular}
\end{table}

The Fitzpatrick sensitivity gap of 0.044 indicates that the maximum difference in true positive rate across skin types is less than 5 percentage points---a substantial improvement over models trained without fairness-aware sampling, which typically exhibit gaps exceeding 15\% \citep{daneshjou2022disparities}.

\subsection{Robustness to Image Distortions}

Real-world smartphone photos exhibit blur, noise, compression artifacts, and poor lighting. We evaluated model robustness across 12 distortion types on 1,000 test images. Table~\ref{tab:robustness} compares XGBoost on frozen vs. fine-tuned embeddings.

\begin{table}[h]
\centering
\caption{Accuracy under various image distortions. $\Delta$ shows the improvement from fine-tuned embeddings.}
\label{tab:robustness}
\begin{tabular}{lccc}
\toprule
\textbf{Distortion} & \textbf{XGB (Frozen)} & \textbf{XGB (Fine-tuned)} & \textbf{$\Delta$} \\
\midrule
None (clean) & 0.962 & 0.975 & +1.3\% \\
Blur (light) & 0.918 & 0.953 & +3.5\% \\
Blur (heavy) & 0.897 & 0.929 & +3.2\% \\
Brightness (dark) & 0.859 & 0.901 & +4.2\% \\
Brightness (bright) & 0.868 & 0.901 & +3.3\% \\
Compression (light) & 0.950 & 0.968 & +1.8\% \\
Compression (heavy) & 0.948 & 0.970 & +2.2\% \\
Rotation (15$^\circ$) & 0.903 & 0.940 & +3.7\% \\
Rotation (45$^\circ$) & 0.889 & 0.927 & +3.8\% \\
Combined (realistic) & 0.843 & 0.858 & +1.5\% \\
\bottomrule
\end{tabular}
\end{table}

Fine-tuned embeddings provide consistent robustness improvements, with the largest gains for blur (+3.5\%), brightness changes (+4.2\%), and rotation (+3.7\%)---common issues in smartphone photography. Notably, both models struggle with heavy noise, suggesting an area for future improvement.

\subsection{Cross-Domain Generalization}

Table~\ref{tab:domain} shows performance stratified by imaging domain.

\begin{table}[h]
\centering
\caption{XGBoost performance by imaging domain.}
\label{tab:domain}
\begin{tabular}{lccccc}
\toprule
\textbf{Domain} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{Specificity} & \textbf{AUC} & \textbf{n} \\
\midrule
Clinical & 0.980 & 0.955 & 0.984 & 0.990 & 3,404 \\
Dermoscopic & 0.940 & 0.954 & 0.936 & 0.986 & 5,592 \\
Smartphone & 0.989 & 0.986 & 1.000 & 1.000 & 460 \\
\bottomrule
\end{tabular}
\end{table}

Notably, performance on smartphone images---the most realistic deployment scenario---is excellent (98.9\% accuracy), demonstrating successful domain generalization from the multi-dataset training approach.

\subsection{Per-Dataset Performance}

Table~\ref{tab:dataset_perf} shows performance on each constituent dataset.

\begin{table}[h]
\centering
\caption{XGBoost performance by source dataset.}
\label{tab:dataset_perf}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{AUC} & \textbf{n} \\
\midrule
HAM10000 & 0.940 & 0.945 & 0.984 & 2,021 \\
DDI & 0.955 & 0.971 & 0.992 & 134 \\
Fitzpatrick17k & 0.981 & 0.954 & 0.990 & 3,270 \\
PAD-UFES-20 & 0.989 & 0.986 & 1.000 & 460 \\
BCN20000 & 0.940 & 0.958 & 0.987 & 3,571 \\
\bottomrule
\end{tabular}
\end{table}

Performance on DDI (0.955 accuracy, 0.971 sensitivity) is particularly notable, as this dataset was specifically designed to evaluate fairness across skin tones with biopsy-confirmed labels.

\subsection{Model Sizes and Inference Times}

Deployment in low-resource settings requires consideration of computational requirements. Table~\ref{tab:efficiency} presents model sizes and inference latencies.

\begin{table}[h]
\centering
\caption{Model sizes and inference times (measured on RTX 4070 Ti SUPER GPU / Intel CPU).}
\label{tab:efficiency}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Size} & \textbf{Time} \\
\midrule
Fine-tuned SigLIP (full) & 3,350 MB & 48 ms (GPU) \\
SigLIP embedding extraction & 3,350 MB & 16 ms (GPU) \\
XGBoost classifier & 1.4 MB & 0.6 ms (CPU) \\
Classification head only & 1.1 MB & --- \\
\bottomrule
\end{tabular}
\end{table}

For web deployment with GPU, the full fine-tuned SigLIP provides the simplest inference pipeline at 48ms per image. For CPU-only or edge deployment, pre-extracting embeddings (16ms GPU) then running XGBoost (0.6ms CPU) achieves comparable accuracy with lower total latency when batching is possible. The 1.4 MB XGBoost model is practical for mobile deployment when paired with a compressed vision backbone.

\subsection{Condition Classification}

For the 10-class condition estimation task, logistic regression on frozen embeddings achieves 68.4\% accuracy and 0.596 F1 macro. Per-condition F1 scores range from 0.584 (seborrheic keratosis) to 0.864 (vascular lesion), with melanoma at 0.667. This secondary output provides users with additional context but is not intended for diagnosis.

\subsection{Mobile Deployment via Knowledge Distillation}

For offline mobile deployment in low-resource settings, we apply knowledge distillation \citep{hinton2015distilling} from the fine-tuned SigLIP teacher (3.4 GB) to lightweight student models.

\begin{table}[h]
\centering
\caption{Knowledge distillation results. Student models trained on 47,277 images using soft labels from fine-tuned SigLIP.}
\label{tab:distillation}
\begin{tabular}{lrrccc}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{Size} & \textbf{Accuracy} & \textbf{F1 Macro} & \textbf{AUC} \\
\midrule
SigLIP (Teacher) & 878M & 3.4 GB & 92.3\% & 0.887 & 0.960 \\
MobileNetV3-Large & 3.2M & 12.5 MB & 92.4\% & 0.884 & 0.959 \\
EfficientNet-B0 & 4.3M & 16.8 MB & 92.7\% & 0.887 & 0.960 \\
\bottomrule
\end{tabular}
\end{table}

Both distilled models match teacher performance within 0.4\% F1 while reducing model size by 200--270$\times$. The models are exported to ONNX format for conversion to Core ML (iOS) and TFLite (Android), enabling fully offline inference on consumer smartphones. With INT8 quantization, model sizes can be further reduced to 5--6 MB.

\section{Discussion}

\subsection{Domain Robustness Through Multi-Dataset Training}

Our results demonstrate that training on diverse imaging domains enables robust generalization. The strong performance on smartphone images (98.9\% accuracy) is particularly encouraging for deployment in low-resource settings where dermoscopic equipment is unavailable.

The key insight is that domain-balanced sampling prevents the model from learning spurious correlations between imaging conditions and diagnoses. Without such balancing, models trained predominantly on dermoscopic images learn that dermoscope-specific artifacts (e.g., polarized lighting patterns, standardized framing) correlate with pathology, leading to failure on clinical photographs.

\subsection{Fairness-Performance Trade-offs}

Fairness-aware sampling does not substantially compromise overall performance. Our Fitzpatrick-balanced model achieves comparable accuracy to models trained without balancing, while reducing sensitivity gaps across skin types from >15\% to <5\%. This suggests that fairness and performance are not fundamentally in tension---the apparent trade-off in prior work may reflect insufficient data diversity rather than an inherent limitation.

\subsection{Two-Stage Training: Fine-Tune Embeddings, Then Boost}

A key finding is that the best results come from a two-stage approach: (1) fine-tune the SigLIP vision encoder on our dermatology data, then (2) train XGBoost on the fine-tuned embeddings. This outperforms both end-to-end fine-tuning with a neural classification head (96.8\% vs 96.2\%) and gradient boosting on frozen embeddings (96.8\% vs 95.7\%).

We hypothesize that fine-tuning improves the embedding space by learning dermatology-specific features, while XGBoost's ensemble of decision trees provides robustness to noise and outliers that neural classification heads lack. The robustness experiments support this: fine-tuned embeddings provide consistent improvements under distortions, and XGBoost maintains high accuracy even with degraded inputs.

\subsection{Foundation Models for Medical Imaging}

SigLIP's strong zero-shot performance on dermatology tasks supports the hypothesis that vision-language foundation models capture generalizable visual features useful for medical imaging. Fine-tuning only the last 4 transformer layers (of 27 total) is sufficient to achieve state-of-the-art performance, suggesting that earlier layers encode domain-general visual features that transfer well.

\subsection{Limitations}

Several limitations should be noted:

\textbf{Dataset bias}: Despite aggregating five datasets, our training data remains biased toward lighter skin tones and certain geographic regions. Performance on skin types V-VI, while improved, may still lag behind lighter types.

\textbf{Label noise}: Fitzpatrick17k labels are derived from web-scraped images with automated labeling, introducing potential noise. DDI and PAD-UFES-20 provide biopsy-confirmed labels but are smaller.

\textbf{Clinical validation}: Our evaluation is retrospective on curated datasets. Prospective clinical validation is required before deployment \citep{freeman2020algorithm, navarrete2021automated}.

\textbf{Triage vs. diagnosis}: SkinTag is designed for triage, not diagnosis. The system cannot replace professional dermatological evaluation.

\textbf{Image framing and distance}: Training datasets contain close-up, well-framed lesion images (typically 2--10cm from skin). Real-world smartphone photos may be captured from farther distances, contain multiple lesions, or have the region of interest poorly framed. This domain gap can significantly degrade performance \citep{korner2022smartphone}. We address this through:
\begin{enumerate}
    \item UI guidance prompting users to center and zoom on the lesion
    \item Field condition augmentations that include scale variations
    \item Recommended future work on automated lesion detection for cropping
\end{enumerate}

\textbf{Condition-specific sensitivity gaps}: While overall binary triage performance is strong (AUC 0.992), per-condition analysis reveals gaps for high-priority conditions: melanoma sensitivity is 68\% (target: 95\%), SCC sensitivity is 59\% (target: 93\%). Future work will apply hierarchical multi-task fine-tuning with clinical class weights to close these gaps.

\subsection{Ethical Considerations}

Deploying AI for medical screening raises significant ethical concerns. We emphasize that SkinTag is intended as a triage aid, not a diagnostic tool. All outputs include prominent disclaimers, and the system explicitly recommends professional evaluation for any concerning findings.

The fairness improvements we demonstrate are encouraging but incomplete. Continued monitoring for disparate impact across demographic groups is essential, particularly if the system is deployed in populations underrepresented in training data.

\section{Conclusion}

We presented SkinTag, a skin lesion triage system that addresses domain shift and skin tone bias through multi-dataset training with fairness-aware sampling. Our key contributions are:

\begin{enumerate}
    \item \textbf{State-of-the-art performance}: XGBoost on fine-tuned SigLIP embeddings achieves 96.8\% accuracy and 0.922 F1-malignant on a diverse 5-dataset benchmark, outperforming both end-to-end fine-tuning and classical ML on frozen embeddings.

    \item \textbf{Fairness}: Equalized odds sensitivity gap below 5\% across Fitzpatrick skin types, achieved without sacrificing overall performance.

    \item \textbf{Robustness}: Consistent improvements under blur, brightness variation, compression, and rotation---common issues in smartphone photography---with fine-tuned embeddings providing 3-4\% accuracy gains over frozen embeddings under distortion.

    \item \textbf{Clinical triage system}: A four-tier urgency classification (URGENT/PRIORITY/ROUTINE/MONITOR) with condition-specific guidance, designed with dermatologist input for actionable screening recommendations.

    \item \textbf{Mobile deployment}: Knowledge distillation reduces model size by 200--270$\times$ (from 3.4 GB to 12--17 MB) while preserving accuracy, enabling offline inference on consumer smartphones.

    \item \textbf{Two-stage training insight}: Fine-tuning improves the embedding space for dermatology-specific features, while gradient boosting provides robustness that neural classification heads lack.
\end{enumerate}

Our results suggest that the apparent trade-off between fairness and performance in dermatology AI may be an artifact of insufficient data diversity. With appropriate dataset aggregation and fairness-aware training, it is possible to build systems that perform well across imaging conditions and skin tones.

Future work will focus on (1) prospective clinical validation, (2) hierarchical multi-task fine-tuning to improve condition-specific sensitivity for high-priority conditions (melanoma, SCC), (3) automated lesion detection for robust handling of poorly-framed images, and (4) expansion to additional underrepresented populations.

\section*{Acknowledgments}

We thank the creators of the HAM10000, DDI, Fitzpatrick17k, PAD-UFES-20, and BCN20000 datasets for making their data publicly available for research.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
